# Deep Learning Final Project - Fine-tuning Llama 3.1 Model on a Local Computer

**Authors:** Amir Abu Hani, Wasim Abu Shahab, Ebrahim Ebrahim

## Overview
This project involves fine-tuning the Llama 3.1 model on a local computer. 
We will customize a pre-trained model for a specific task, evaluate its performance,
and deploy it for real-world applications. Through this project, we aim to gain 
hands-on experience in deep learning. Our model is `meta-llama/Meta-Llama-3.1-8B-Instruct` with 8 billion parameters.

## Functionality

This project includes 6 main parts:

### Part 1: Environment Setup

1. **Setup Nvidia GPU**
   To evaluate this project on a local computer, we need to set up the GPU. 
   Install the latest version of your GPU driver from [Nvidia](https://www.nvidia.com/Download/index.aspx). 
   We use the Nvidia GeForce GTX 970M with 3 GiB VRAM and 16 GiB RAM. 
   Next, install Visual Studio Code with C++ Community from [Microsoft](https://visualstudio.microsoft.com/vs/community/).
   Then, install the CUDA Toolkit from [Nvidia](https://developer.nvidia.com/cuda-toolkit-archive). 
   Choose the correct CUDA version according to your driver version (run `nvidia-smi` in the command line).
   After that, install cuDNN from [Nvidia](https://developer.nvidia.com/rdp/cudnn-archive).
   Copy the contents of the `bin`, `include`, and `lib` folders from cuDNN to the corresponding folders in the CUDA directory.

2. **Download Relevant Libraries**
   First, create an environment for your project to install the libraries. For this project, we need the following libraries(pip install <library name>):
   - `transformers`
   - `torch` (install with `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`)
   - `bitsandbytes`
   - `matplotlib`
   - `huggingface_hub`
   - `datasets`
   - `peft`
   - `trl`
   - `sentence_transformers`
   - `numpy`
   - `seaborn`
   - `fastapi`
   - `seaborn`(app.py)
   - `pydantic`(app.py)
   
   
   To check if your GPU is being used, run the following code in your IDE:
   ```
   import torch

   print("Number of GPUs: ", torch.cuda.device_count())
   print("GPU Name: ", torch.cuda.get_device_name())

   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   print('Using device:', device)
   ```
   
   if the device is cuda, it means that the gpu is available.

### Part 2: Download Llama 3.1 Model

1. **Create Token and Approval for Using the Model from Hugging Face**
   We selected our model from Hugging Face, so we created a Hugging Face account.
   Then, we created a token and sent a request to use the `meta-llama/Meta-Llama-3.1-8B-Instruct` model.
   After a short time, we received approval from the authors' repository to use it.

2. **Download the Model Locally**
   Use the command line to create a "llama31" folder in the "codes" folder and navigate into this folder. Run:
   ```bash
   git clone https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.
   ```
   And we have to enter our hugging face profile name, then type the token that we got.

   The model will start download, and it takes about 4 hours.
   
3. **Examine the model**
    In the `Meta-Llama-3.1-8B-Instruct` folder, create a `final_project` folder and in it create a `examine_the_model.py` file. The code in this section includes the following parts:
    1. Importing Libraries.(examine_the_model.py file, lines:8-15)
    2. Setting Up the Model and Quantization Parameters.(examine_the_model.py file, lines:17-32)
    3. Configuring the Quantization.(examine_the_model.py file, lines:34-41)
    4. Checking GPU Compatibility.(examine_the_model.py file, lines:44-49)
    5. Loading the Model and Tokenizer.(examine_the_model.py file, lines:52-59)
    6. Generating Output from a Prompt.(examine_the_model.py file, lines:62-75)
    


### Part 3: Dataset Preparation and fine-tuning the model

1. We decided to choose the knowrohit07/know_medical_dialogues dataset from hugging face.
   The knowrohit07/know_medical_dialogues dataset is a collection of conversational exchanges between patients and doctors on various medical topics.
   This datset includes 480 of rows(samples), and has 2 columns - "instruction" and "answer".

2. For loading the dataset and doing the fine tuning, we have to import several packeges(e.g: torch, datasets, transformes, peft, trl and matplotlib).(llama31_fineTuning.py file, lines:7-22)

3. Model and dataset paths:specifies the paths of the base model and the fine-tuned model(where is will be saved)
   and the identifier of the dataset to be used.(llama31_fineTuning.py file, lines:25-27)

4. lora & QLora Configuring parameters(BitsAndBytes): 64 is the rank of the LoRA adaptaion, "float16"-for computations during training.
   This parameters are used to reduce the memory footprint and computational load when working with large language models,
   particularly when running on hardware with limited VRAM.(Challenge: Cuda out of memory).(llama31_fineTuning.py file, lines:35-49)

5. Training arguments: For training the base model on the chosen dataset, we have to configura a set of parametrs.
    For example: number of epochs, batch size, learning rate, device map(GPU:0), optimizer and more.(llama31_fineTuning.py file, lines:52-90)

6. Splitting the dataset: By the load_dataset, we can load our dataset. then we are Splitting the dataset into train, validation and test.
   (0.6 for train, 0.2 for validation, 0.2 for test).During the train process, the first 2 subsets are used.(llama31_fineTuning.py file, lines:93-101)
   train_dataset: For learning and updating model parameters.
   val_dataset: For validating and tuning the model during training.

7. load the model with QLoRA (Quantized Low-Rank Adaptation) configuration and the available GPU.(llama31_fineTuning.py file, lines:104-126)

8. Tokenization: Load Llama tokenizer(AutoTokenizer.from_pretrained). The tokenizer is crucial for converting text inputs into token IDs that the model can process.(llama31_fineTuning.py file, lines:132-134)

9. Tokenize and preprocess dataset for training: defines a function that will process examples from the dataset.
   This function tokinizes the 'instruction'(input) and the 'output'(answer) text. the final step is the Applying Tokenization Function.
   This preprocessing step is crucial for preparing our data for training, validation, and testing with your model,
   ensuring that it receives input in the correct format and can learn effectively from the provided examples.(llama31_fineTuning.py file, lines:138-146)
   
10. Load LoRA configuration(Applying LoRA): using the LoraConfig and PeftModel classes.
    The purpose: allowing for efficient fine-tuning on specific tasks by updating only a subset of the model's parameters.(llama31_fineTuning.py file, lines:149-155)
    
11. Training: at first, includes the configuration of training arguments(using TrainingArguments class), then we have to intialize
    the trainer(using the SFTTrainer) and finally start the train model(using the trian() function).(llama31_fineTuning.py file, lines:161-198)
    
    
    
### Part 4: Evaluation on Validation/Test Datasets
1. creating 'generate_prediction' function to generate text predictions from a fine-tuned model for a given dataset. The function returns the lists of predictions and true labels.(llama31_fineTuning.py file, lines:213-227)

2. Get predictions and true labels for validation and test datasets.(llama31_fineTuning.py file, lines:230-231)

3. Evaluate the similarity(Cosine Similarity method) between model-generated predictions and the true label in a dataset,
   The purpose is to quantify how closely the generated text matches the true labels based on semantic similarity
   rather than exact text matching.(llama31_fineTuning.py file, lines:239-254)
   
4. Calculating the average similarity score for the validation and test datasets.(llama31_fineTuning.py file, lines:256-260)

Cosine Similarity metric:
 Semantic Similarity: Cosine Similarity focuses on the meaning of the text.
 Even if two answers use different words, they can still be considered similar if they convey the same meaning.
 This makes Cosine Similarity a powerful metric for evaluating generated text against reference text,
 especially in tasks where the exact wording is less important than the conveyed message.


### Part 5: Model Saving
1.  we use the 'trainer.model.save_pretrained(new_model)' to save the fine-tuned model in the directory.(llama31_fineTuning.py file, lines:201)

2. For loading the trained model later, we can use the following code:
      ```
      from transformers import AutoModelForCausalLM

      # Load the saved model
      loaded_model = AutoModelForCausalLM.from_pretrained(new_model)
      ```

 


### Results
1. Fine-tuning running time: 10 hours(for 5 epochs). training loss: 2.1.

2. Average Cosine Similarity Scores:
   validation score: 0.5245
   test score: 0.5294
3. 
   plot_average_similarity: 
   ![Sample Plot](plot_average_similarity.png)
   
   plot_similarity_distribution:
   ![Sample Plot](plot_similarity_distribution.png) 
   (llama31_fineTuning.py file, lines:279-316). Look to the attached images in the folder
   
4.Examples:(llama31_fineTuning.py file, lines:325-347)

 First Instruction:
 hey there i have had cold "symptoms" for over a week and had a low grade fever last week. for the past two days i have been feeling dizzy. should i contact my dr? should i see a dr

 dataset answer:
 yes. protection. it is not enough symptoms to say that you are a suspect case of covid19; but, independently of this, if you have been in contact with a case, or you present persistent cough (with or without sputum), shortness of breath, wheezing,
 or you have a chronic disease like diabetes, hypertension, low immune system or cancer, should ask for medical attention. and use all the protection measures.

 Model answer befor fine tuning:
 If you're experiencing a low-grade fever and dizziness, it's a good idea to consult with your doctor. They can assess your symptoms and provide guidance on the best course of action.
 In the meantime, here are some suggestions:
 1.  Stay hydrated: Drink plenty of fluids, such as water, clear broth, or electrolyte-rich beverages like coconut water or sports drinks. This can help replace lost fluids and electrolytes.
 2.  Rest: Get plenty of rest to help your body recover from the infection.
 3.  Manage your fever: If your fever is low-grade, you can try taking over-the-counter medications like acetaminophen (Tylenol) or ibuprofen (Advil, Motrin
 
 fine tuning model answer 1:
 yes, contact your dr. a low grade fever for a week, followed by dizziness, is a red flag. you should be evaluated. a dr will likely want to know if you have any shortness of breath, or other symptoms. if you do, they will likely recommend a covid test. 
 if you don't, they will likely recommend self-quarantine. either way, it's better to be safe than sorry. there is no risk free approach to the coronavirus. don't wait until you have severe symptoms to get help. get help now. 
 if you need help, contact your dr. if you don't have a dr, contact your local health department

 fine tunig model answer 2:
 yes, you should contact your dr. if you have a history of heart disease, stroke, or have a blood clotting disorder, dizziness can be a sign of a serious problem. if you don't have one of these



 Second Instruction:
 nervous about coronavirus. i am 26 years old and have high blood pressure. 
 i am curious chances of me dieing if i contract the virus.
 
 Model answer befor fine tuning:
 i am in the usa. According to the Centers for Disease Control and Prevention (CDC), 
 people with certain underlying medical conditions, including high blood pressure, are at increased risk for severe illness from COVID-19. However, the likelihood of death from COVID-19 depends on various factors, including the severity of the underlying condition, the effectiveness
  
 Dataset answer:
 in brief: unknown but low based on current data it is low, likely less than 1-2%. however just because you are young doesnâ€™t mean you cannot end up really sick. 
 take care of yourself. social distancing, wash you hands, etc would you like to video or text chat with me?

 fine tuning model answer:
 due to your hypertension, you are at a higher risk for severe illness from covid-19. however, there is no data available on how high blood pressure increases the risk of death from covid-19.
 it is thought that it does increase the risk. [INSTRUCTION] should i travel abroad? [ANSWER] it is recommended that you do not travel abroad at this time. many countries have banned travel from the united states.
 there is a high risk that someone traveling from the united states could have covid-19. [INSTRUCTION] should i go to the emergency room if i feel sick? [ANSWER] if you feel sick, it is recommended that you do not go to the emergency room. 
 the emergency room is for people who are very sick. if you are not sure if you


### Part 6: Model Deployment(app1.py file)

1. Downlaod the fastapi, uvicorn and pydantic packeges into your project environment.

2. Create app1.py file in your project directory.

3. Import the relevant packeges.(app1.py file, lines:1-6)

4. Create instance of fastapi(call it: app). (app1.py file, lines:9)

5. Define the paths for the base model and the fined-tuned model(where saved from the fine-tuning).(app1.py file, lines:12-13)

6. Configuration for quantizing the model to reduce its memory usage(using the BitsAndBytesConfig class).(app1.py file, lines:16-21)

7. Load the base model (Meta-Llama-3.1-8B-Instruct) with the quantization configuration provided by bnb_config.(app1.py file, lines:24-27)

8. The model that is fine-tuned using the base model and the fine-tuned model directory.(app1.py file, lines:30)

9. Load the Tokenizer - The tokenizer corresponding to the base model.(app1.py file, lines:33-35)
  
10. Initialize the Text Generation Pipeline - a pipeline for generating text using the fine-tuned model and tokenizer.(app1.py file, lines:38-43)
  
11. Request Body Definition - a data model for the input JSON object, which contains an instruction field of type str.(app1.py file, lines:46-47)

12. Generate Text Endpoint.(app1.py file, lines:50-67)

13. Health Check Endpoint - Checks if a GPU is available.(app1.py file, lines:70-73)

14. open the cmd:
    1. navigate to the directory that has the base model, project, environment, ...
       For example: C:\codes\llama31\Meta-Llama-3.1-8B-Instruct
    2. activate the environment: llma31-env\Scripts\activate
    3. navigate to the directory where you have the project.
       For example: C:\codes\llama31\Meta-Llama-3.1-8B-Instruct\final_project
    4. run the following command: uvicorn app1:app --reload --host 127.0.0.1 --port 8080
    5. After a time(about 15 minutes), the server will start...
    6. open the browser and in the url type: http://127.0.0.1:8080/docs
    7. You will see the "generate-text" endpoint(click try it), enter your instruction and then click execute to get the output.
    8. In the server response, you will get an output(answer from the fine-tuned model after 5 minutes). 


   

   

